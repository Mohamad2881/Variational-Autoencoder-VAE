import torch
from torch import nn

import numpy as np
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

from typing import Tuple, Optional, List


def load_checkpoint(checkpoint, model, optimizer):
    print("=> Loading checkpoint")
    model.load_state_dict(checkpoint["state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer"])
    epoch = checkpoint["epoch"]
    return epoch


def vae_loss_fn(imgs: torch.Tensor, z_mean: torch.Tensor, z_log_var: torch.Tensor, decoded: torch.Tensor,
                weight: float = 1) -> torch.Tensor:
    """Calculate the loss for a VAE model.
       Loss Formula: loss = `weight` * reconstruction loss + KL divergence loss
    Args:
        imgs (torch.Tensor): The input images to the VAE model.
        z_mean (torch.Tensor): The mean vector of the latent variables.
        z_log_var (torch.Tensor): The log variance of the latent variables.
        decoded (torch.Tensor): The output images generated by the VAE model.
        weight (float): The weight to apply to the reconstruction loss. Defaults to 1.

    Returns:
        torch.Tensor: The total loss for the VAE model.
    """

    # calculate KL divergence loss
    # Sum across latent dimension
    KL_div_loss = -0.5 * torch.sum((z_log_var + 1 - z_mean ** 2 - torch.exp(z_log_var)), dim=1)

    # Take average across batch_size dimension
    KL_div_loss = KL_div_loss.mean()

    # Calculaate reconstruction loss
    loss_fn = nn.functional.mse_loss

    # calculate mse pixel wise and sum over all pixels and take mean across batch size
    recon_loss = loss_fn(imgs, decoded, reduction='none')
    recon_loss = recon_loss.view(imgs.size(0), -1).sum(1).mean()

    # calculate total loss as a weighted sum of KL divergence and reconstruction loss
    loss = weight * recon_loss + KL_div_loss

    return loss


def generate_images_from_decoder(model: torch.nn.Module, latent_dim: int, num_images: int,
                                 device: torch.device) -> torch.Tensor:
    """Generate new images from a given model using decoder only and random latent vectors.

    Args:
        model (torch.nn.Module): The model to use for generating the images.
        latent_dim (int): The dimension of the latent space used by the decoder model.
        num_images (int): The number of images to generate.
        device (torch.device): The device to use for generating the images (e.g. 'cpu' or 'cuda').

    Returns:
        torch.Tensor: A tensor containing the generated images.
    """

    # move model to device and set to evaluation mode
    model = model.to(device)
    model.eval()

    # generate random latent vectors and use them to generate new images
    with torch.inference_mode():
        random_z = torch.randn(num_images, latent_dim).to(device)
        new_imgs = model.decoder(random_z)

    return new_imgs


def generate_images_from_vae(model: torch.nn.Module, images: torch.Tensor, device: torch.device) -> torch.Tensor:
    """Generate new images from a given VAE model using both encoder and decoder and a set of input images.

    Args:
        model (torch.nn.Module): The VAE model to use for generating the images.
        images (torch.Tensor): The input images to use as a basis for generating new images.
        device (torch.device): The device to use for generating the images (e.g. 'cpu' or 'cuda').

    Returns:
        torch.Tensor: A tensor containing the generated images.
    """

    # move model and images to device and set to evaluation mode
    model = model.to(device)
    images = images.to(device)
    model.eval()

    # generate new images using the input images as a basis
    with torch.inference_mode():
        _, _, _, new_imgs = model(images)

    return new_imgs


def encode_images(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader,
                  device: torch.device) -> torch.Tensor:
    """Encode images using a given model and return their embeddings.

    Args:
        model (torch.nn.Module): The model to use for encoding the images.
        dataloader (DataLoader): The dataloader containing the images to encode.
        device (str): The device to use for encoding the images (e.g. 'cpu' or 'cuda').

    Returns:
        torch.Tensor: A tensor containing the embeddings of the encoded images.
    """

    # move model to device and set to evaluation mode
    model = model.to(device)
    model.eval()

    # create an empty tensor to store the embeddings
    all_embeddings = torch.Tensor().to(device)

    # encode each batch of images and concatenate the resulting embeddings
    for (images, _) in tqdm(dataloader):
        images = images.to(device)

        with torch.inference_mode():
            _, z, _, _ = model.encode(images)

        all_embeddings = torch.cat((all_embeddings, z))

    return all_embeddings.cpu()


def plot_imgs(imgs: torch.Tensor, nrows: Optional[int] = None, ncols: int = 5, title: Optional[str] = None,
              alphas: Optional[List[float]] = None, cmap: Optional[str] = None) -> None:
    """Plot a list of images.

    Args:
        imgs (torch.Tensor): The input images to plot.
        nrows (int, optional): The number of rows to use in the plot. Default is None, which calculates nrows based on ncols and the number of images in the input tensor.
        ncols (int, optional): The number of columns to use in the plot. Default is 5.
        title (str, optional): The title of the plot. Default is None.
        alphas (List[float], optional): A list of alpha values to use as titles for each image. Default is None.
        cmap (str, optional): The colormap to use for displaying the images. Default is None.

    Returns:
        None
    """

    if alphas is not None and len(alphas) != len(imgs):
        raise ValueError("alphas should be the same length as imgs or None.")

    # calculate number of rows if not provided
    if nrows is None:
        nrows = int(np.ceil(len(imgs) // ncols))

    # send imgs to cpu if needed
    if imgs.is_cuda:
        imgs = imgs.cpu()

    # Make channels last for plotting
    imgs = imgs.permute(0, 2, 3, 1)

    # create a new figure with subplots
    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 4, nrows * 4))
    fig.suptitle(title, fontsize=30)

    # plot each image with its corresponding title
    for i, (img, ax) in enumerate(zip(imgs, axs.flatten())):
        ax.imshow(img, cmap=cmap)
        ax.axis(False)
        if alphas:
            t = 'Original' if alphas[i] == 0 else f'$\\alpha=${alphas[i]}'
            ax.set_title(t, fontsize=25)

    # adjust spacing between subplots and figure title
    fig.subplots_adjust(top=0.8)


def plot_latent_dims(embeddings: torch.Tensor, nrows: int = None, ncols: int = 10, title: str = None) -> None:
    """Plot histograms of the values in each dimension of the input embeddings.

    Args:
        embeddings (torch.Tensor): The input embeddings to plot.
        nrows (int, optional): The number of rows to use in the plot. Default is None, which calculates nrows based on ncols and the number of dimensions in the input embeddings.
        ncols (int, optional): The number of columns to use in the plot. Default is 10.
        title (str, optional): The title of the plot. Default is None.

    Returns:
        None
    """
    # send imgs to cpu if needed
    if embeddings.is_cuda:
        embeddings = embeddings.cpu()

    # calculates nrows based on ncols and the number of dimensions in the input embeddings.
    if nrows is None:
        nrows = int(np.ceil(embeddings.shape[1] // ncols))

    # create a new figure with subplots
    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 4, nrows * 4))
    fig.suptitle(title, fontsize=30)

    # plot histograms of each dimension of the embeddings
    for (i, ax) in enumerate(axs.flatten()):
        ax.hist(embeddings[:, i])


def calculate_avg(data_loader: torch.utils.data.DataLoader, img_dim: Tuple[int, int, int],
                  device: torch.device) -> torch.Tensor:
    """Calculate the average image from a data loader.
    Iterates over all images in a dataloader an calculates their average image.

    Args:
        data_loader (DataLoader): The data loader containing the images to use.
        img_dim (Tuple[int, int, int]): The dimensions of the images (channels, height, width).
        device (torch.device): The device to use for the computation.

    Returns:
        torch.Tensor: The calculated average image as a tensor.
    """
    # Initialize the average image with zeros
    avg_img = torch.zeros(img_dim).to(device)

    # Total number of images in the `data_loader`.
    num_imgs = len(data_loader.dataset)

    # sum of all images in the data loader
    for images, _ in tqdm(data_loader):
        images = images.to(device)
        avg_img += torch.sum(images, axis=0)

    # Divide by the total number of imgs to get the average
    avg_img /= num_imgs

    return avg_img.cpu()


def manipulate_img(img: torch.Tensor, diff: torch.Tensor, alphas: Tuple[float, ...] = (0., 0.5, 1., 1.5, 2., 2.5, 3.),
                   latent_space: bool = False, model: Optional[torch.nn.Module] = None,
                   device: Optional[torch.device] = None) \
        -> Tuple[torch.Tensor, torch.Tensor]:
    """Manipulate an image by adding or removing features in either pixel space or latent space.

    Args:
        img (torch.Tensor): The input image to manipulate.
        diff (torch.Tensor): The difference vector that defines the feature to add or remove.
        alphas (tuple[float], optional): A tuple of alpha values to use when manipulating the image.
            Default is (0., 0.5, 1., 1.5, 2., 2.5, 3.).
        latent_space (bool, optional): Whether to manipulate the image in latent space or pixel space.
            Default is False.
        model (nn.Module, optional): The model to use for encoding and decoding the image in latent space.
            Required if `latent_space` is True.
        device (torch.device, optional): The device to use for encoding and decoding the image in latent space.
            Required if `latent_space` is True.

    Returns:
        tuple[torch.Tensor, torch.Tensor]: A tuple containing two tensors of manipulated images (more_feature_imgs, less_feature_imgs).
    """

    if (latent_space and (model is None or device is None)) or \
            (model is not None and (not latent_space or device is None)) or \
            (device is not None and (not latent_space or model is None)):
        raise ValueError("Both latent_space, model and device must be provided if using any one of them.")

    # Initialize tensors to hold the results
    more_feature_imgs = torch.Tensor()
    less_feature_imgs = torch.Tensor()

    # check Whether to manipulate the image in latent space or pixel space
    if latent_space:
        model = model.to(device)
        model.eval()
        diff = diff.to(device)
        # encode image to get a latent embedding vector
        with torch.inference_mode():
            _, latent_vec, _, _ = model.encode(img.unsqueeze(0).to(device))

    # Manipulate image for each alpha
    for alpha in alphas:
        if latent_space:
            # maniipulate latent vector
            more_feature_vec = latent_vec + alpha * diff
            less_feature_vec = latent_vec - alpha * diff
            with torch.inference_mode():
                # decode latent vector after adding or removing feature
                more_feature_img = model.decoder(more_feature_vec).cpu()
                less_feature_img = model.decoder(less_feature_vec).cpu()

        # Manipulate image in pixel space
        else:
            more_feature_img = (img + alpha * diff).unsqueeze(0)
            less_feature_img = (img - alpha * diff).unsqueeze(0)

        # Concatenate the manipulated images to output tensors
        more_feature_imgs = torch.cat((more_feature_imgs, more_feature_img))
        less_feature_imgs = torch.cat((less_feature_imgs, less_feature_img))

    return more_feature_imgs, less_feature_imgs